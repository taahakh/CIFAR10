{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1710885323897,"user":{"displayName":"Ahnaf Khalique","userId":"10133885603065877505"},"user_tz":0},"id":"oVgYc6HEHd1f"},"outputs":[],"source":["import torch\n","from torch import nn\n","from torch.utils.data import DataLoader\n","from torchvision.datasets import CIFAR10\n","import torchvision.transforms as transforms\n","from IPython import display\n","import matplotlib.pyplot as plt\n","import numpy as np\n","from enum import Enum"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Get the device for training using either CPU or GPU\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","print(f'Running on: {device}')"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1710885326267,"user":{"displayName":"Ahnaf Khalique","userId":"10133885603065877505"},"user_tz":0},"id":"vYZzibNnLjU1"},"outputs":[],"source":["# [STEP 1] - Getting dataset and creating dataloaders \n","\n","# Function to get the CIFAR10 dataset and create the dataloaders\n","# Batch size is set to 16 by default\n","def get_data(batch_size=16):\n","\n","    # Data augmentation, normalization for training and creating the tensors\n","    transform_train = transforms.Compose([\n","        transforms.RandomHorizontalFlip(),\n","        transforms.RandomCrop(32, 4),\n","        transforms.ToTensor(),\n","        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n","    ])\n","\n","    # Test (validation) transform matching the normalisation as the same as the training data and creating the tensors\n","    transform_test = transforms.Compose([\n","        transforms.ToTensor(),\n","        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n","    ])\n","\n","    # Downloading the CIFAR10 dataset and applying the transformations\n","    train_dataset = CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n","    test_dataset = CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n","\n","    # Creating the dataloaders and setting the batch size\n","    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n","\n","    return train_loader, test_loader"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# [STEP 2] - Creating the model \n","\n","# Block\n","# This class defines the blocks of the backbone of the network\n","class Block(nn.Module):\n","  def __init__(self,\n","               in_channels, # Inital channel value is three, updated with output channel of each block for next block\n","               out_channels, # Output channel of the block\n","               conv_layer=3, # Number of conv layers in the block\n","               conv_kernel=3, # Kernel size of the conv layers\n","               conv_stride=1, # Stride of the conv layers\n","               conv_padding=1, # Padding of the conv layers\n","               max_pool=False, # If the block has max pooling or not\n","               ):\n","\n","    super(Block, self).__init__()\n","\n","    # Number of conv layers in the block\n","    self.k = conv_layer\n","\n","    # Spatial Average Pooling - for the Linear layer to produce weights\n","    self.sap = nn.AdaptiveAvgPool2d((1, 1)).to(device)\n","\n","    # Linear layer generating weights for the conv2d layers\n","    self.Linear1 = nn.Linear(in_channels, self.k).to(device)\n","\n","    # Non-linear activation\n","    # Used for the Linear layer and the conv2d layers\n","    self.g = nn.ReLU(inplace=True).to(device)\n","\n","    #Flatten - reshape dimensions for Linear layer\n","    self.flat = nn.Flatten().to(device)\n","\n","    # Maximum Pooling\n","    # Block may have this layer or not, dependent on what the practioner has set\n","    # Reduces the spatial dimensions of the weight*conv2d tensor\n","    self.max_pool = nn.MaxPool2d(2, 2).to(device) if max_pool else None\n","\n","    # Create k-conv layers\n","    for i in range(self.k):\n","      self.add_module(\n","          'conv{0}'.format(i),\n","          nn.Conv2d(in_channels,\n","                    out_channels,\n","                    kernel_size=conv_kernel,\n","                    stride=conv_stride,\n","                    padding=conv_padding\n","                    ).to(device)\n","      )\n","\n","    # Batch norm layers - one for each conv layer\n","    self.bn_layers = nn.ModuleList([nn.BatchNorm2d(out_channels).to(device) for _ in range(self.k)])\n","    \n","  def forward(self, x):\n","      # ---- WEIGHT GENERATION ----\n","      out = self.sap(x)       # Spatial Average Pooling\n","      out = self.flat(out)    # Flatten\n","      out = self.Linear1(out) # Linear layer\n","      out = self.g(out)       # Non-linear activation\n","\n","      # ---- WEIGHTS * CONV2DS ----\n","      batch_size = out[:, 0].size(0)\n","      block_out = 0\n","      for i in range(self.k):\n","        # Conv2d --> BatchNorm --> ReLU\n","        conv = self._modules['conv{0}'.format(i)](x)\n","        conv = self.bn_layers[i](conv)\n","        conv = self.g(conv)\n","        # Get the i'th weights (and reshaped to be compatible for multiplication) and multiply it with the i'th conv2d layer\n","        block_out += out[:, i].view(batch_size, 1, 1, 1) * conv\n","      \n","      # ---- REDUCE SPATIAL DIMENSIONS ----\n","      if self.max_pool:\n","        block_out = self.max_pool(block_out)\n","\n","      return block_out\n","\n","# Network - Backbone and Classifier\n","# This class defines the network architecture, generating the backbone and classifier\n","class Net(nn.Module):\n","    def __init__(self, blocks): # Blocks is a list of tuples, each tuple contains the parameters for the block\n","        super(Net, self).__init__()\n","        self.num_classes = 10 # CIFAR10 has 10 classes\n","\n","        # ---- GENERATING BLOCKS AND BACKBONE ----\n","        backbone_blocks = []\n","        channel_pass = 3 # Intial channel is three; inital channel is updated with output channel of each block for next block\n","        for conv_layer, out_channels, k, s, p, mxpool in blocks:\n","            backbone_blocks.append(\n","                Block(\n","                    channel_pass,\n","                    out_channels,\n","                    conv_layer=conv_layer,\n","                    conv_kernel=k,\n","                    conv_stride=s,\n","                    conv_padding=p,\n","                    max_pool=mxpool\n","                ).to(device)\n","            )\n","            channel_pass = out_channels\n","        \n","        self.out_channels = channel_pass # Output channel of the last block\n","\n","        # Adding the blocks to the backbone      \n","        self.backbone = nn.Sequential(*backbone_blocks)\n","\n","        # ---- CREATING CLASSIFIER ----\n","        self.classifier = nn.Sequential(\n","            nn.AdaptiveAvgPool2d((1, 1)),       # f = SpatialAveragePool(On), On is output of last block\n","            nn.Flatten(),                       # Reshape f, ready for and pass to classifier\n","            nn.Linear(self.out_channels, 512),  # classifier steps\n","            nn.ReLU(inplace=True),              # classifier steps\n","            nn.Dropout(0.5),                    # classifier steps\n","            nn.Linear(512, self.num_classes),   # classifier steps\n","        )\n","\n","\n","    def forward(self, x):\n","        return self.classifier(self.backbone(x))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# [STEP 4 - part of] - Calculating and tracking the loss and accuracy, and plotting graphs\n","\n","%matplotlib inline\n","\n","# Enum for the type of run - training or evaluation\n","class RunType(Enum):\n","    TRAIN = 1\n","    EVAL = 2\n","\n","# Calculating and tracking the loss and accuracy, and plotting graphs\n","class Stats():\n","    # total_epochs - number of epochs to train\n","    # target_acc - target accuracy to stop training\n","    def __init__(self, total_epochs, target_acc=0.91):\n","\n","        # Train loss per end of epoch\n","        self.train_loss = []\n","        # Train accuracy per end of epoch\n","        self.train_acc = []\n","        # Test loss per end of epoch (evaluation)\n","        self.test_loss = []\n","        # Test accuracy per end of epoch (evaluation)\n","        self.test_acc = []\n","\n","        self.total_epochs = total_epochs\n","        self.target = target_acc\n","\n","        # List of the epochs accuracy, loss, learning rate stats to print for each epoch\n","        self.print_list = []\n","\n","        # Running loss, correct predictions and total predictions for training\n","        self.average_loss = 0.0\n","        self.running_loss = 0.0\n","        self.correct = 0\n","        self.total = 0\n","\n","        # Running loss, correct predictions and total predictions for evaluation\n","        self.eval_average_loss = 0.0\n","        self.eval_running_loss = 0.0\n","        self.eval_correct = 0\n","        self.eval_total = 0\n","    \n","    # Update training accuracy and loss during training / evaluation\n","    # Referenced from: https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html\n","    # run_type - training or evaluation, run_loss - loss for the current batch, y_hat - predicted values, y - actual values\n","    def update_loss_accuracy(self, run_type, run_loss, y_hat, y):\n","        if run_type == RunType.TRAIN:\n","            self.running_loss += run_loss # Accumulate loss for each batch in the epoch \n","            _, pred = torch.max(y_hat, 1) # Extracts the predicted class labels  \n","            self.total += y.size(0) # Accumulate total number of predictions\n","            self.correct += (pred == y).sum().item() # Accumulate correct predictions\n","        elif run_type == RunType.EVAL:\n","            self.eval_running_loss += run_loss\n","            _, pred = torch.max(y_hat, 1)\n","            self.eval_total += y.size(0)\n","            self.eval_correct += (pred == y).sum().item()\n","    \n","    # Calculate and keep track of average loss and accuracy for each epoch for training / evaluation\n","    # run_type - training or evaluation, num_batches - number of batches \n","    def calculate_loss_accuracy(self, run_type, num_batches):\n","        avg_loss, avg_acc = 0, 0 \n","        if run_type == RunType.TRAIN:\n","            self.average_loss = self.running_loss / num_batches   # Loss of the current epoch\n","            avg_loss = self.average_loss\n","            avg_acc = self.correct / self.total                 # Accuracy of the current epoch\n","            self.train_loss.append(self.average_loss)\n","            self.train_acc.append(avg_acc)\n","            self.correct, self.total, self.running_loss = 0, 0, 0 # Reset for the next epoch\n","        elif run_type == RunType.EVAL:\n","            self.eval_average_loss = self.eval_running_loss / num_batches\n","            avg_loss = self.eval_average_loss\n","            avg_acc = self.eval_correct / self.eval_total\n","            self.test_loss.append(self.eval_average_loss)\n","            self.test_acc.append(avg_acc)\n","            self.eval_correct, self.eval_total, self.eval_running_loss = 0, 0, 0\n","\n","        return avg_loss, avg_acc\n","\n","    # Plots accuracy and loss graphs, and prints stats for each epoch\n","    # final_epoch is used to set the final, true x-axis limit for the graphs once training has finished \n","    def print_stats(self, final_epoch=None):\n","        epochs = range(1, len(self.train_loss) + 1)\n","\n","        # Plot training loss\n","        plt.figure(figsize=(8, 6))\n","        plt.plot(epochs, self.train_loss, label='Training Loss')\n","        plt.plot(epochs, self.test_loss, label='Validation Loss')\n","        plt.title('Training and Validation Loss')\n","        plt.xlabel('Epoch')\n","        plt.ylabel('Loss')\n","        plt.legend()\n","        plt.ylim(0, 3)\n","        plt.xlim(0, self.total_epochs if final_epoch is None else final_epoch+10) # Adding 10 padding for better visualisation\n","        plt.grid(True)\n","\n","        # Plot training accuracy\n","        plt.figure(figsize=(8, 6))\n","        plt.plot(epochs, self.train_acc, label='Training Accuracy')\n","        plt.plot(epochs, self.test_acc, label='Validation Accuracy')\n","        plt.title('Training and Validation Accuracy')\n","        plt.xlabel('Epoch')\n","        plt.ylabel('Accuracy')\n","        plt.legend()\n","        plt.ylim(0, 1)\n","        plt.xlim(0, self.total_epochs if final_epoch is None else final_epoch+10)\n","        plt.yticks(np.arange(0, 1+0.1, 0.1))\n","        plt.grid(True)\n","\n","        plt.show()\n","        display.display(self.print_list)\n","        display.clear_output(wait=True)\n","\n","    # Print stats at the end of each epoch - accuracy, loss, learning rate\n","    def update(self, epoch, lr):\n","        self.print_list.append(f'Epoch: {epoch}, Loss: {self.average_loss}, Test Loss: {self.eval_average_loss}, Train Accuracy: {self.train_acc[-1]}, Test (Evaluation) Accuracy: {self.test_acc[-1]},  Next LR: {lr}')\n","        self.print_stats()\n","    \n","    # Print finalised stats once accuracy has been reached / training/evaluation has finished - accuracy, loss, learning rate\n","    def finished(self, final_epoch):\n","        self.print_list.append(f'Finished training at epoch: {final_epoch} --> Evaluation accuracy: {self.test_acc[-1]*100}%')\n","        self.print_stats(final_epoch)\n","    \n","    # Check if the inital training accuracy is poor\n","    # Used to restart and reinitalise the model if the training accuracy is poor\n","    def is_poor_performance(self):\n","        return self.train_acc[-1] < 0.2 and len(self.train_acc) == 1 \n","\n","    # Check if the target accuracy has been reached\n","    def has_reached_target(self):\n","        return self.test_acc[-1] >= self.target"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# [STEP 4 - part of] - Training and evaluation script\n","\n","# Training the model        \n","def _train(net, train_iter, loss, optimiser, device, stats):\n","    net.train()\n","    \n","    # Standard training loop\n","    for X, y in train_iter:\n","        optimiser.zero_grad()\n","        \n","        X, y = X.to(device), y.to(device)\n","        y_hat = net(X)\n","        l = loss(y_hat, y)\n","        \n","        l.backward()\n","        optimiser.step()\n","\n","        # Update training loss and accuracy, and track these values\n","        stats.update_loss_accuracy(RunType.TRAIN, l.item(), y_hat, y)\n","    \n","    # Calculate loss and accuracy, and update stats for each epoch\n","    stats.calculate_loss_accuracy(RunType.TRAIN, len(train_iter))\n","\n","# Evaluate the model\n","# net - model, test_iter - evaluation data, loss - loss function, device - CPU or GPU, stats - stats object to keep track of loss, accuracy and learning rate\n","def _evaluate_accuracy(net, test_iter, loss, device, stats):\n","    net.eval()\n","\n","    # No gradient calculation for evaluation\n","    with torch.no_grad():\n","        # Pass to network and get loss\n","        for X, y in test_iter:\n","            X, y = X.to(device), y.to(device)\n","            y_hat = net(X)\n","\n","            # Calculate loss\n","            eval_loss = loss(y_hat, y)\n","\n","            # Update training loss and accuracy, and track these values\n","            stats.update_loss_accuracy(RunType.EVAL, eval_loss.item(), y_hat, y)\n","\n","        # Calculate loss and accuracy, and update stats for each epoch\n","        avg_loss, _ = stats.calculate_loss_accuracy(RunType.EVAL, len(test_iter))\n","\n","    return avg_loss\n","\n","# Train Model\n","# net - model, train_iter - training data, test_iter - evaluation data, loss - loss function, optimiser - optimiser, scheduler - learning rate scheduler, device - CPU or GPU, epochs - number of epochs\n","def train_model(net, train_iter, test_iter, loss, optimiser, scheduler, device, epochs=60):\n","    \n","    # Stats object to keep track of loss, accuracy and learning rate\n","    stats = Stats(epochs)\n","    current_epoch = 0 # Store the last epoch for the final stats\n","    for epoch in range(epochs):\n","        current_epoch = epoch\n","\n","        # Training\n","        _train(net, train_iter, loss, optimiser, device, stats)\n","\n","        # Evaluation\n","        eval_loss = _evaluate_accuracy(net, test_iter, loss, device, stats)\n","        \n","        # Update learning rate with evaluation loss after each epoch\n","        scheduler.step(eval_loss)\n","\n","        # View current stats\n","        stats.update(epoch, scheduler.get_last_lr())\n","\n","        # Rejects training if inital training accuracy is poor - for reinitialisation and restarting the training\n","        # ONLY USED FOR THE FIRST EPOCH TRAINING ACCURACY\n","        if stats.is_poor_performance():\n","            return False\n","\n","        # Breaks the training loop if the target accuracy has been reached (minimum 91%)\n","        if stats.has_reached_target():\n","            break\n","    \n","    # Finished training - print final stats\n","    stats.finished(current_epoch)\n","    \n","    return True\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# [STEP 3, 4, 5] -  Training\n","\n","def start_training():\n","\n","    # 6 BLOCKS for the backbone\n","    # Parameters: num_conv_layer, conv_channels, conv_kernel, conv_stride, conv_padding, do_max_pooling\n","    blocks = (\n","        (3, 64, 3, 1, 1, False),\n","        (3, 64, 3, 1, 1, True),\n","        (3, 128, 3, 1, 1, False),\n","        (3, 128, 3, 1, 1, True),\n","        (3, 256, 3, 1, 1, False),\n","        (3, 256, 3, 1, 1, True),\n","    )\n","\n","    batch_size = 16\n","    lr, wd = 0.0001, 1e-5  # Learning rate and weight decay\n","    \n","    # [STEP 3] - Create loss and optimiser\n","    # Also initalising the model with the blocks, getting the dataloaders and scheduler\n","    trainloader, testloader     =   get_data(batch_size)\n","    model                       =   Net(blocks).to(device)\n","    loss                        =   nn.CrossEntropyLoss().to(device)\n","    optimiser                   =   torch.optim.Adam(model.parameters(), lr=lr, weight_decay=wd)\n","    sched                       =   torch.optim.lr_scheduler.ReduceLROnPlateau(optimiser, mode='min', patience=5)\n","\n","    # ------ IMPORTANT ------\n","    # Sometimes the model will start with accuracy scores closely around 0.1 and RARELY will not train properly after that i.e. gets stuck at 0.1 essentially\n","    # In current testing however, this issue has not been encountered as it should be fixed now with lower learning rate but may be possible\n","    # Weight initialisation makes it worse. So the approach was to leave it to pytorch to automatically initalise the weights\n","    # We want the starting accuracy (train accuracy on first epoch) to be at least 20% for safe measure but can be lower and still train properly\n","    # Therefore, this while loop will restart the training process if the training accuracy of the first epoch is less than 20%\n","    # It rarely will restart, if so, it should restart once.\n","    # If there is an issue (hopefully not and extremely unlikely), please restart the kernel and run the training again\n","    # The model will train for a maximum of 80 epochs, but will stop early once it reaches 91% accuracy\n","    # It usually reaches 91% accuracy around 20 - 50 epochs\n","    while not train_model(model, trainloader, testloader, loss, optimiser, sched, device, 80):\n","        print('Poor performance start on first epoch training accuracy (< 0.2)')\n","        print('Reinitialising model, loss, optimiser, scheduler, data loaders')\n","        \n","        # Relieve memory (RAM / GPU VRAM)\n","        del model\n","        del loss\n","        del optimiser\n","        del sched\n","        del trainloader\n","        del testloader\n","\n","        # Refreshing and reinitialising the model, loss, optimiser, scheduler, data loaders\n","        trainloader, testloader     =   get_data(batch_size)\n","        model                       =   Net(blocks).to(device)\n","        loss                        =   nn.CrossEntropyLoss().to(device)\n","        optimiser                   =   torch.optim.Adam(model.parameters(), lr=lr, weight_decay=wd)\n","        sched                       =   torch.optim.lr_scheduler.ReduceLROnPlateau(optimiser, mode='min', patience=5)\n","        \n","        print('Reinitialisation complete, restarting training')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":16658,"status":"error","timestamp":1710885423101,"user":{"displayName":"Ahnaf Khalique","userId":"10133885603065877505"},"user_tz":0},"id":"cyiF3VMNRtat","outputId":"90781ae3-b238-4626-8afb-b10a4672a1e2"},"outputs":[],"source":["# Start training\n","start_training()"]}],"metadata":{"accelerator":"TPU","colab":{"authorship_tag":"ABX9TyMiIxqKIalNuMc3F+S5nxMS","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.6"}},"nbformat":4,"nbformat_minor":0}
